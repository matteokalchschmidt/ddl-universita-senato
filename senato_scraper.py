import osimport jsonfrom pathlib import Pathimport refrom datetime import datetime, datefrom typing import List, Dict, Optionalfrom urllib.parse import urlencodeimport requestsfrom bs4 import BeautifulSoupimport pdfplumberfrom pathlib import Pathimport jsonfrom urllib.parse import urljoin# --- NUOVE FUNZIONI PER L'ITER ---def estrai_sezione_testo(soup, titolo_sezione):    """    Cerca un <h2>/<h3> con il titolo della sezione (es. 'Iter', 'Presentazione')    e concatena il testo dei nodi successivi fino al prossimo h2/h3.    """    titolo_norm = titolo_sezione.strip().lower()    for h in soup.find_all(["h2", "h3"]):        testo_h = h.get_text(" ", strip=True).lower()        if testo_h.startswith(titolo_norm):            parti = []            for sib in h.find_next_siblings():                if sib.name in ["h2", "h3"]:                    break                # saltiamo cose troppo “strane”                if sib.name in ["script", "style"]:                    continue                txt = sib.get_text(" ", strip=True)                if txt:                    parti.append(txt)            if parti:                return "\n".join(parti).strip()            return None    return Nonedef estrai_iter_da_scheda(scheda_url):    """    Scarica la scheda DDL del Senato e ne estrae:    - Iter    - Presentazione    - Assegnazione    Restituisce un dizionario con questi campi (alcuni possono essere None).    """    print(f"[INFO] Scarico scheda DDL: {scheda_url}")    resp = requests.get(scheda_url, timeout=20)    resp.raise_for_status()    soup = BeautifulSoup(resp.text, "html.parser")    iter_txt = estrai_sezione_testo(soup, "Iter")    presentazione_txt = estrai_sezione_testo(soup, "Presentazione")    assegnazione_txt = estrai_sezione_testo(soup, "Assegnazione")    return {        "scheda_url": scheda_url,        "iter": iter_txt,        "presentazione": presentazione_txt,        "assegnazione": assegnazione_txt,    }BASE_URL = "https://www.senato.it"LIST_URL = (    "https://www.senato.it/leggi-e-documenti/disegni-di-legge/"    "testi-dei-disegni-di-legge?page=0&size=1")# Parole chiave per identificare i DDL rilevanti per il sistema universitarioUNIV_KEYWORDS = [    "università",    "universitar",  # universitar* (prende universitaria, universitarie, ecc.)    "ateneo",    "atenei",    "afam",    "alta formazione artistica",    "ricercator",  # ricercatore, ricercatori    "ricerca scientifica",    "dottorat",  # dottorato, dottorandi    "assegni di ricerca",    "borse di studio",    "studenti universitari",    "corsi di laurea",    "laurea magistrale",    "professori universitari",    "personale docente e ricercatore",    "abilitazione scientifica nazionale",    "abilitazione all'insegnamento universitario",]LEGISLATURA = 19  # XIX legislatura# Mappa (da completare) fra numero DDL e URL della scheda con l'iterDDL_SCHEDA_URLS = {    # ESEMPI già pronti:    # S. 66 - Nuove norme in materia di DSA    "66": "https://www.senato.it/leggi-e-documenti/disegni-di-legge/scheda-ddl?did=55227",    # S. 148 - Ricercatori nelle università e negli enti pubblici di ricerca    "148": "https://www.senato.it/leggi-e-documenti/disegni-di-legge/scheda-ddl?did=55310",    # S. 259 - Abolizione numero chiuso all'università    "259": "https://www.senato.it/leggi-e-documenti/disegni-di-legge/scheda-ddl?did=55920",    # qui potrai aggiungere altri DDL man mano:    # "317": "...",    # "844": "...",    # ecc.}def is_university_related(title: str) -> bool:    t = title.lower()    return any(kw in t for kw in UNIV_KEYWORDS)def fetch_page_html() -> str:    """Scarica la pagina principale con l'elenco dei testi dei DDL."""    resp = requests.get(LIST_URL, timeout=30)    resp.raise_for_status()    return resp.textdef parse_bill_line(text: str) -> Optional[Dict]:    """    Dato un testo tipo:    '66 Nuove norme in materia di disturbi specifici di apprendimento in ambito scolastico, universitario e lavorativo 13/10/2022'    estrae:    - numero: '66'    - titolo: 'Nuove norme in materia di disturbi specifici di apprendimento in ambito scolastico, universitario e lavorativo'    - data: datetime.date    """    # Togli eventuale '(xxx KB)' in fondo    text = re.sub(r"\(\s*\d+\s*KB\s*\)$", "", text).strip()    # Trova l'ultima data nel formato dd/mm/yyyy    date_match = list(re.finditer(r"\d{2}/\d{2}/\d{4}", text))    if not date_match:        return None    last_date_m = date_match[-1]    date_str = last_date_m.group(0)    try:        date_obj = datetime.strptime(date_str, "%d/%m/%Y").date()    except ValueError:        return None    # Parte prima della data    before_date = text[: last_date_m.start()].strip()    # Il numero (es. '66' o '13-B') è la prima "parola"    parts = before_date.split(maxsplit=1)    if not parts:        return None    # Esempi: "317," -> "317" ; "1518-A" resta "1518-A"    numero = parts[0].rstrip(",;.")    titolo = parts[1].strip() if len(parts) > 1 else ""    return {        "numero": numero,        "titolo": titolo,        "data": date_obj,    }def fetch_bills() -> List[Dict]:    """    Ritorna una lista di DDL con:    - numero    - titolo    - data    - pdf_url    """    html = fetch_page_html()    soup = BeautifulSoup(html, "html.parser")    bills: List[Dict] = []    # Tutti i link al PDF (estensione .pdf)    for a in soup.find_all("a", href=True):        href = a["href"].strip()        # Consideriamo solo i link che puntano a PDF        if ".pdf" not in href.lower():            continue        # --- NORMALIZZAZIONE HREF DEL SENATO ---        # Caso: href del tipo "/www.senato.it/service/PDF/..."        if "/www.senato.it/" in href:            # teniamo solo da "/service/..." in poi            idx = href.index("/www.senato.it/") + len("/www.senato.it")            href = href[idx:]  # es: "/service/PDF/..."        # Caso: href del tipo "//www.senato.it/service/..."        if href.startswith("//"):            href = "https:" + href  # diventa https://www.senato.it/service/...        # Costruiamo l'URL assoluto        if href.startswith("http://") or href.startswith("https://"):            pdf_url = href        elif href.startswith("/"):            pdf_url = BASE_URL + href  # es: https://www.senato.it/service/...        else:            pdf_url = BASE_URL + "/" + href.lstrip("/")        # --- TESTO DELLA RIGA (per numero, titolo, data) ---        # Di solito il link al PDF è in una cella <td>        # e numero/titolo/data sono nella stessa riga <tr>        row = a.find_parent("tr")        if row is not None:            text = row.get_text(" ", strip=True)        else:            # Fallback: contenitore diretto            container = a.parent            if container is None:                continue            text = container.get_text(" ", strip=True)        bill_info = parse_bill_line(text)        if bill_info is None:            continue        bill_info["pdf_url"] = pdf_url        bill_info["legislatura"] = LEGISLATURA        bills.append(bill_info)    return billsdef build_scheda_url(numero: str, legislatura: int = 19) -> str:    """    Costruisce l'URL (ipotesi) della scheda 'Atto Senato n. {numero}'.    Esempio:    https://www.senato.it/leggi-e-documenti/disegni-di-legge/scheda-ddl?leg=19&numero=66    """    params = {"leg": legislatura, "numero": numero}    return f"{BASE_URL}/leggi-e-documenti/disegni-di-legge/scheda-ddl?{urlencode(params)}"def extract_section_text(soup: BeautifulSoup, heading_label: str) -> str:    """    Cerca una sezione del tipo 'Iter', 'Presentazione', 'Assegnazione' nella scheda DDL    e ne restituisce il testo come stringa unica.    Logica:    - trova un tag con testo che contiene 'heading_label' (es. 'Iter')      tra h2/h3/h4 o <dt>    - prende i fratelli successivi fino al prossimo heading della stessa famiglia    """    # Cerca un heading o un <dt> che contenga la label    heading = soup.find(        lambda tag: tag.name in ["h2", "h3", "h4", "dt"]        and heading_label.lower() in tag.get_text(strip=True).lower()    )    if not heading:        return ""    # Prende i fratelli successivi finché non incontra un nuovo heading    texts = []    sibling = heading.find_next_sibling()    while sibling and sibling.name not in ["h2", "h3", "h4", "dt"]:        txt = sibling.get_text(" ", strip=True)        if txt:            texts.append(txt)        sibling = sibling.find_next_sibling()    return " ".join(texts).strip()def fetch_iter_for_bill(numero: str, legislatura: int = 19) -> Optional[Dict[str, str]]:    """    Scarica la scheda DDL 'Atto Senato n. {numero}' e prova a estrarre:    - iter    - presentazione    - assegnazione    Restituisce un dizionario con queste chiavi (stringhe, anche vuote),    oppure None se la scheda non è raggiungibile.    """    url = build_scheda_url(numero, legislatura)    try:        resp = requests.get(url, timeout=30)    except requests.RequestException as e:        print(f"[WARN] Errore di rete per DDL {numero}: {e}")        return None    if resp.status_code != 200:        print(f"[WARN] Scheda DDL {numero} non raggiungibile (HTTP {resp.status_code}) - URL: {url}")        return None    soup = BeautifulSoup(resp.text, "html.parser")    iter_text = extract_section_text(soup, "Iter")    presentazione_text = extract_section_text(soup, "Presentazione")    assegnazione_text = extract_section_text(soup, "Assegnazione")    # Se non troviamo nulla, segnaliamo ma restituiamo comunque qualcosa    if not any([iter_text, presentazione_text, assegnazione_text]):        print(f"[WARN] Nessuna sezione 'Iter/Presentazione/Assegnazione' trovata per DDL {numero} (controllare HTML reale).")    return {        "url_scheda": url,        "iter": iter_text,        "presentazione": presentazione_text,        "assegnazione": assegnazione_text,    }def save_university_bills_to_json(univ_bills: List[Dict], output_path: Path) -> None:    """    Salva la lista dei DDL universitari in un file JSON.    Converte la data in stringa ISO (YYYY-MM-DD) per essere serializzabile.    """    serializable = []    for b in univ_bills:        item = dict(b)        if isinstance(item.get("data"), datetime):            item["data"] = item["data"].date().isoformat()        elif item.get("data") is not None:            # se è già una date, la convertiamo in stringa            item["data"] = item["data"].isoformat()        serializable.append(item)    output_path.parent.mkdir(parents=True, exist_ok=True)    with output_path.open("w", encoding="utf-8") as f:        json.dump(serializable, f, ensure_ascii=False, indent=2)def normalizza_spazi(s: str) -> str:    """Riduce spazi multipli e spazi prima della punteggiatura."""    if not s:        return ""    s = re.sub(r"\s+", " ", s)    s = re.sub(r"\s+([,;:.])", r"\1", s)    return s.strip()def tronca_frase(testo: str, max_len: int = 200) -> str:    """    Tronca a max_len caratteri cercando di fermarsi a una fine frase.    Se non trova un punto, tronca secco.    """    if not testo:        return ""    testo = testo.strip()    if len(testo) <= max_len:        return testo    # Prova a trovare un punto entro max_len    cut = testo.rfind(".", 0, max_len)    if cut == -1:        # Nessun punto: tronca secco e aggiungi "..."        return testo[: max_len - 3].rstrip() + "..."    return testo[: cut + 1].strip()def sintetizza_iter(bill: dict) -> str:    """    Crea una breve sintesi dell'iter usando i campi di iter_info.    Non usa alcuna AI, solo operazioni su stringhe.    """    iter_info = bill.get("iter_info") or {}    presentazione = iter_info.get("presentazione") or ""    assegnazione = iter_info.get("assegnazione") or ""    iter_txt = iter_info.get("iter") or ""    parti = []    # 1) Sintesi presentazione    if presentazione:        p = normalizza_spazi(presentazione)        # Proviamo a estrarre la data dopo 'Presentato in data'        m = re.search(r"Presentato in data\s+(\d{1,2}\s+\w+\s+\d{4})", p)        if m:            parti.append(f"Presentato il {m.group(1)}.")        else:            # altrimenti usiamo la prima frase troncata            parti.append(tronca_frase(p, max_len=150))    # 2) Sintesi assegnazione    if assegnazione:        a = normalizza_spazi(assegnazione)        # prendiamo la prima frase (o due) fino al primo punto        parti.append("Assegnazione: " + tronca_frase(a, max_len=200))    # 3) Sintesi stato iter    if iter_txt:        i = normalizza_spazi(iter_txt)        # togliamo la parte "Successione delle letture parlamentari..."        i = re.split(r"Successione delle letture parlamentari", i)[0].strip()        parti.append("Stato iter: " + tronca_frase(i, max_len=200))    return " ".join(parti)def scarica_pdf(pdf_url: str, cartella: Path, nome_file: str) -> Path:    """    Scarica il PDF da pdf_url e lo salva in `cartella` con nome_file.    Restituisce il Path del file salvato.    """    cartella.mkdir(parents=True, exist_ok=True)    destinazione = cartella / nome_file    if destinazione.exists():        # non riscarichiamo se già presente        return destinazione    print(f"[INFO] Scarico PDF: {pdf_url}")    resp = requests.get(pdf_url, timeout=60)    resp.raise_for_status()    with open(destinazione, "wb") as f:        f.write(resp.content)    return destinazionedef estrai_testo_da_pdf(percorso_pdf: Path) -> str:    """    Estrae il testo da un PDF usando pdfplumber.    """    testo = []    with pdfplumber.open(percorso_pdf) as pdf:        for pagina in pdf.pages:            txt = pagina.extract_text() or ""            if txt:                testo.append(txt)    return "\n\n".join(testo).strip()def analizza_struttura_testo(testo: str) -> dict:    """    Produce una mini-sintesi strutturale:    - 'incipit': le prime 5-8 righe del testo    - 'articoli': elenco dei titoli di articolo trovati    """    righe = [r.strip() for r in testo.splitlines() if r.strip()]    incipit = "\n".join(righe[:8])    articoli = []    for r in righe:        if re.match(r"^(Art\.?|Articolo)\s*\d+", r):            articoli.append(r)    return {        "incipit": incipit,        "articoli": articoli,    }MESI_IT = {    "gennaio": 1,    "febbraio": 2,    "marzo": 3,    "aprile": 4,    "maggio": 5,    "giugno": 6,    "luglio": 7,    "agosto": 8,    "settembre": 9,    "ottobre": 10,    "novembre": 11,    "dicembre": 12,}def estrai_data_da_testo(testo: str) -> str | None:    """    Cerca la prima data con formato '18 luglio 2023' e la converte in '2023-07-18'.    Restituisce None se non trova nulla.    """    if not testo:        return None    match = re.search(r"(\d{1,2})\s+([A-Za-zàéèìòù]+)\s+(\d{4})", testo)    if not match:        return None    giorno = int(match.group(1))    nome_mese = match.group(2).lower()    anno = int(match.group(3))    mese = MESI_IT.get(nome_mese)    if mese is None:        return None    return f"{anno:04d}-{mese:02d}-{giorno:02d}"def estrai_eventi_iter(iter_txt: str) -> list[dict]:    """    A partire dal testo dell'iter (es. '18 luglio 2023: ritirato ...'),    estrae una lista di eventi:    [      {"data": "2023-07-18", "descrizione": "ritirato"},      ...    ]    La logica è semplice: ogni riga tipo '12 marzo 2025: ...' diventa un evento.    """    eventi = []    if not iter_txt:        return eventi    for riga in iter_txt.splitlines():        riga = riga.strip()        if not riga:            continue        m = re.match(r"(\d{1,2}\s+[A-Za-zàéèìòù]+\s+\d{4})\s*:\s*(.+)", riga)        if not m:            continue        data_str = m.group(1)        descr = m.group(2).strip()        data_iso = estrai_data_da_testo(riga)        eventi.append({            "data": data_iso,            "descrizione": descr,        })    # Ordiniamo per data se disponibile    eventi_ordinati = sorted(        eventi,        key=lambda e: (e["data"] is None, e["data"] or "")    )    return eventi_ordinatidef analizza_iter(iter_txt: str) -> dict:    """    Analizza l'iter e restituisce:    - stato: RITIRATO / APPROVATO / IN_CORSO / ASSEGNATO / SCONOSCIUTO    - data_ultimo_evento: 'YYYY-MM-DD' se trovata, altrimenti None    - eventi: lista (eventi cronologici)    - testo_grezzo: l'iter completo    """    if not iter_txt:        return {            "stato": "SCONOSCIUTO",            "data_ultimo_evento": None,            "eventi": [],            "testo_grezzo": "",        }    testo_lower = iter_txt.lower()    # CLASSIFICAZIONE SEMPLICE DELLO STATO    if "ritirato" in testo_lower:        stato = "RITIRATO"    elif "approvato definitivamente" in testo_lower or "legge n." in testo_lower:        stato = "APPROVATO"    elif "in corso di esame" in testo_lower:        stato = "IN_CORSO"    elif "assegnato" in testo_lower:        stato = "ASSEGNATO"    else:        stato = "SCONOSCIUTO"    # ESTRAZIONE EVENTI E DATA ULTIMO EVENTO    eventi = estrai_eventi_iter(iter_txt)    data_ultimo_evento = eventi[-1]["data"] if eventi and eventi[-1]["data"] else estrai_data_da_testo(iter_txt)    return {        "stato": stato,        "data_ultimo_evento": data_ultimo_evento,        "eventi": eventi,        "testo_grezzo": iter_txt,    }def classifica_tipologia_e_ambito(bill: dict) -> dict:    """    Ritorna un dizionario con:    - tipologia: tipo di provvedimento (euristica sul titolo)    - ambito: area tematica principale (università, ricerca, medicina, AFAM, ecc.)    """    titolo = (bill.get("titolo") or "").lower()    # --- Tipologia ---    if "conversione in legge" in titolo:        tipologia = "DECRETO_LEGGE"    elif "bilancio di previsione" in titolo or "rendiconto generale" in titolo:        tipologia = "BILANCIO"    elif "delega al governo" in titolo:        tipologia = "DELEGA"    elif "modifica alla legge" in titolo or "modifiche alla legge" in titolo:        tipologia = "MODIFICA_LEGGE"    else:        tipologia = "ORDINARIO"    # --- Ambito ---    ambito = "ALTRO"    if "università" in titolo or "universitari" in titolo or "studenti universitari" in titolo:        ambito = "UNIVERSITA"    if "ricerca" in titolo:        ambito = "RICERCA"    if "medicine e chirurgia" in titolo or "medicina e chirurgia" in titolo or "professioni sanitarie" in titolo:        ambito = "MEDICINA"    if "afam" in titolo or "alta formazione artistica" in titolo:        ambito = "AFAM"    if "scuola" in titolo or "alunni" in titolo or "studenti" in titolo:        # se non è già università, mettiamo SCUOLA        if ambito == "ALTRO":            ambito = "SCUOLA"    return {        "tipologia": tipologia,        "ambito": ambito,    }def main():    print("Scarico elenco DDL dal sito del Senato...")    bills = fetch_bills()    print(f"Trovati {len(bills)} testi di DDL (con PDF).")    # Filtriamo i DDL legati a università/ricerca (come già facevi)    bills_uni = [    b for b in bills    if "titolo" in b and isinstance(b["titolo"], str) and is_university_related(b["titolo"])]    print(f"Trovati {len(bills_uni)} DDL legati al sistema universitario (per parole chiave nel titolo).")    # --- ARRICCHIMENTO ITER ---    for bill in bills_uni:        cls = classifica_tipologia_e_ambito(bill)        bill["tipologia"] = cls["tipologia"]        bill["ambito"] = cls["ambito"]        numero = bill["numero"]        scheda_url = DDL_SCHEDA_URLS.get(numero)        if scheda_url:            iter_info = estrai_iter_da_scheda(scheda_url)            bill["iter_info"] = iter_info            bill["iter_sintesi"] = sintetizza_iter(bill)            bill["iter_analisi"] = analizza_iter(iter_info.get("iter", ""))    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>    # >>>>>  INCOLLA QUI IL BLOCCO PER SCARICARE TUTTI I PDF  >>>>>    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>    pdf_dir = Path("pdf")    pdf_dir.mkdir(exist_ok=True)    for bill in bills_uni:        numero = str(bill.get("numero"))        pdf_url = bill.get("pdf_url")        if not pdf_url:            continue        try:            nome_file_pdf = f"ddl_{numero}.pdf"            scarica_pdf(pdf_url, pdf_dir, nome_file_pdf)        except Exception as e:            print(f"[ERRORE] Impossibile scaricare PDF per DDL {numero}: {e}")    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>    # Salviamo il JSON arricchito    # Serializziamo in JSON convertendo le date in stringhe    serializable = []    for b in bills_uni:        item = dict(b)        d = item.get("data")        if isinstance(d, (datetime, date)):            item["data"] = d.isoformat()        serializable.append(item)    os.makedirs("output", exist_ok=True)    output_path = os.path.join("output", "ddl_universita_iter.json")    with open(output_path, "w", encoding="utf-8") as f:        json.dump(serializable, f, ensure_ascii=False, indent=2)    print(f"\nRisultato salvato in: {output_path}")    print("Esempio primo DDL universitario (se presente iter_info):")    if serializable:        print(json.dumps(serializable[0], ensure_ascii=False, indent=2))if __name__ == "__main__":    main()