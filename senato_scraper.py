import jsonfrom pathlib import Pathimport refrom datetime import datetimefrom typing import List, Dict, Optionalfrom urllib.parse import urlencodeimport requestsfrom bs4 import BeautifulSoupBASE_URL = "https://www.senato.it"LIST_URL = (    "https://www.senato.it/leggi-e-documenti/disegni-di-legge/"    "testi-dei-disegni-di-legge?page=0&size=1")# Parole chiave per identificare i DDL rilevanti per il sistema universitarioUNIV_KEYWORDS = [    "università",    "universitar",  # universitar* (prende universitaria, universitarie, ecc.)    "ateneo",    "atenei",    "afam",    "alta formazione artistica",    "ricercator",  # ricercatore, ricercatori    "ricerca scientifica",    "dottorat",  # dottorato, dottorandi    "assegni di ricerca",    "borse di studio",    "studenti universitari",    "corsi di laurea",    "laurea magistrale",    "professori universitari",    "personale docente e ricercatore",    "abilitazione scientifica nazionale",    "abilitazione all'insegnamento universitario",]LEGISLATURA = 19  # XIX legislaturadef is_university_related(title: str) -> bool:    t = title.lower()    return any(kw in t for kw in UNIV_KEYWORDS)def fetch_page_html() -> str:    """Scarica la pagina principale con l'elenco dei testi dei DDL."""    resp = requests.get(LIST_URL, timeout=30)    resp.raise_for_status()    return resp.textdef parse_bill_line(text: str) -> Optional[Dict]:    """    Dato un testo tipo:    '66 Nuove norme in materia di disturbi specifici di apprendimento in ambito scolastico, universitario e lavorativo 13/10/2022'    estrae:    - numero: '66'    - titolo: 'Nuove norme in materia di disturbi specifici di apprendimento in ambito scolastico, universitario e lavorativo'    - data: datetime.date    """    # Togli eventuale '(xxx KB)' in fondo    text = re.sub(r"\(\s*\d+\s*KB\s*\)$", "", text).strip()    # Trova l'ultima data nel formato dd/mm/yyyy    date_match = list(re.finditer(r"\d{2}/\d{2}/\d{4}", text))    if not date_match:        return None    last_date_m = date_match[-1]    date_str = last_date_m.group(0)    try:        date_obj = datetime.strptime(date_str, "%d/%m/%Y").date()    except ValueError:        return None    # Parte prima della data    before_date = text[: last_date_m.start()].strip()    # Il numero (es. '66' o '13-B') è la prima "parola"    parts = before_date.split(maxsplit=1)    if not parts:        return None    numero = parts[0]    titolo = parts[1].strip() if len(parts) > 1 else ""    return {        "numero": numero,        "titolo": titolo,        "data": date_obj,    }def fetch_bills() -> List[Dict]:    """    Ritorna una lista di DDL con:    - numero    - titolo    - data    - pdf_url    """    html = fetch_page_html()    soup = BeautifulSoup(html, "html.parser")    bills = []    # Tutti i link al PDF (estensione .pdf)    for a in soup.find_all("a", href=True):        href = a["href"]        if ".pdf" not in href.lower():            continue        # Costruiamo URL assoluto se è relativo        pdf_url = href        if pdf_url.startswith("/"):            pdf_url = BASE_URL + pdf_url        # 1️⃣ saliamo alla riga di tabella (<tr>) se esiste        row = a.find_parent("tr")        if row is not None:            text = row.get_text(" ", strip=True)        else:            # fallback: usiamo il contenitore diretto (es. <p> o <li>)            container = a.parent            if container is None:                continue            text = container.get_text(" ", strip=True)        bill_info = parse_bill_line(text)        if bill_info is None:            # Se il parse fallisce, passa al prossimo link            continue        bill_info["pdf_url"] = pdf_url        bill_info["legislatura"] = LEGISLATURA        bills.append(bill_info)    return billsdef build_scheda_url(numero: str, legislatura: int = 19) -> str:    """    Costruisce l'URL (ipotesi) della scheda 'Atto Senato n. {numero}'.    Esempio:    https://www.senato.it/leggi-e-documenti/disegni-di-legge/scheda-ddl?leg=19&numero=66    """    params = {"leg": legislatura, "numero": numero}    return f"{BASE_URL}/leggi-e-documenti/disegni-di-legge/scheda-ddl?{urlencode(params)}"def extract_section_text(soup: BeautifulSoup, heading_label: str) -> str:    """    Cerca una sezione del tipo 'Iter', 'Presentazione', 'Assegnazione' nella scheda DDL    e ne restituisce il testo come stringa unica.    Logica:    - trova un tag con testo che contiene 'heading_label' (es. 'Iter')      tra h2/h3/h4 o <dt>    - prende i fratelli successivi fino al prossimo heading della stessa famiglia    """    # Cerca un heading o un <dt> che contenga la label    heading = soup.find(        lambda tag: tag.name in ["h2", "h3", "h4", "dt"]        and heading_label.lower() in tag.get_text(strip=True).lower()    )    if not heading:        return ""    # Prende i fratelli successivi finché non incontra un nuovo heading    texts = []    sibling = heading.find_next_sibling()    while sibling and sibling.name not in ["h2", "h3", "h4", "dt"]:        txt = sibling.get_text(" ", strip=True)        if txt:            texts.append(txt)        sibling = sibling.find_next_sibling()    return " ".join(texts).strip()def fetch_iter_for_bill(numero: str, legislatura: int = 19) -> Optional[Dict[str, str]]:    """    Scarica la scheda DDL 'Atto Senato n. {numero}' e prova a estrarre:    - iter    - presentazione    - assegnazione    Restituisce un dizionario con queste chiavi (stringhe, anche vuote),    oppure None se la scheda non è raggiungibile.    """    url = build_scheda_url(numero, legislatura)    try:        resp = requests.get(url, timeout=30)    except requests.RequestException as e:        print(f"[WARN] Errore di rete per DDL {numero}: {e}")        return None    if resp.status_code != 200:        print(f"[WARN] Scheda DDL {numero} non raggiungibile (HTTP {resp.status_code}) - URL: {url}")        return None    soup = BeautifulSoup(resp.text, "html.parser")    iter_text = extract_section_text(soup, "Iter")    presentazione_text = extract_section_text(soup, "Presentazione")    assegnazione_text = extract_section_text(soup, "Assegnazione")    # Se non troviamo nulla, segnaliamo ma restituiamo comunque qualcosa    if not any([iter_text, presentazione_text, assegnazione_text]):        print(f"[WARN] Nessuna sezione 'Iter/Presentazione/Assegnazione' trovata per DDL {numero} (controllare HTML reale).")    return {        "url_scheda": url,        "iter": iter_text,        "presentazione": presentazione_text,        "assegnazione": assegnazione_text,    }def save_university_bills_to_json(univ_bills: List[Dict], output_path: Path) -> None:    """    Salva la lista dei DDL universitari in un file JSON.    Converte la data in stringa ISO (YYYY-MM-DD) per essere serializzabile.    """    serializable = []    for b in univ_bills:        item = dict(b)        if isinstance(item.get("data"), datetime):            item["data"] = item["data"].date().isoformat()        elif item.get("data") is not None:            # se è già una date, la convertiamo in stringa            item["data"] = item["data"].isoformat()        serializable.append(item)    output_path.parent.mkdir(parents=True, exist_ok=True)    with output_path.open("w", encoding="utf-8") as f:        json.dump(serializable, f, ensure_ascii=False, indent=2)def main():    print("Scarico elenco DDL dal sito del Senato...")    bills = fetch_bills()    print(f"Trovati {len(bills)} testi di DDL (con PDF).")    print("\n--- DDL legati al sistema universitario (per parole chiave nel titolo) ---\n")    univ_bills: List[Dict] = []    for b in bills:        if not is_university_related(b["titolo"]):            continue        univ_bills.append(b)        print(f"DDL n. {b['numero']} - {b['data']}")        print(f"Titolo: {b['titolo']}")        print(f"PDF: {b['pdf_url']}")        print("-" * 80)    if not univ_bills:        print("Nessun DDL universitario trovato (con le parole chiave attuali).")        return    print(f"\nTotale DDL universitari trovati: {len(univ_bills)}")    # Salvataggio su JSON    output_path = Path("output") / "ddl_universita.json"    save_university_bills_to_json(univ_bills, output_path)    print(f"\nRisultati salvati in: {output_path.resolve()}")if __name__ == "__main__":    main()